{"cells":[{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"ename":"ImportError","evalue":"cannot import name 'tags' from 'transformers' (c:\\Users\\maxim\\OneDrive\\proy__transformers\\transformers_env\\lib\\site-packages\\transformers\\__init__.py)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17508\\1639640497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXLMRobertaConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling_outputs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenClassifierOutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling_roberta\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling_roberta\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRobertaPreTrainedModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mImportError\u001b[0m: cannot import name 'tags' from 'transformers' (c:\\Users\\maxim\\OneDrive\\proy__transformers\\transformers_env\\lib\\site-packages\\transformers\\__init__.py)"]}],"source":["import torch.nn as nn\n","from datasets import load_dataset\n","from transformers import XLMRobertaConfig\n","from transformers.modeling_outputs import TokenClassifierOutput\n","from transformers.models.roberta.modeling_roberta import RobertaModel\n","from transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tengo que agregar el codigo de la carga del dataset para ejecutar esto"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n","    # Aca esta cambiando la configuracion de RobertaPretraindeModel\n","    # Se puede ver que es lo que modifica comparando RobertaPretraindeModel.config_class()\n","    config_class = XLMRobertaConfig\n","    \n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","    \n","    # Defino el forward pass\n","    def forward(self, inputs_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n","        # Uso el body del modelo para obtener las representaciones del encoder\n","        outputs = self.roberta(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_iids, **kwargs)\n","\n","        # Aplico el classificador a la representacion de output del encoder\n","        sequence_output = self.dropout(outputs[0])\n","        # Aplico el classifier\n","        logits = self.classifier(sequence_output)\n","        # Calculo las perdidas\n","        loss = None\n","        if labels is not None:\n","            # Inicializo la funcion de perdida\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            return TokenClassifierOutput(loss=loss, logits=logits,\n","                                         hidden_states=outputs.hidden_states,\n","                                         attentions=outputs.attentions)"]}],"metadata":{"kernelspec":{"display_name":"transformers_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b14258719366ad36b7a6821fba4613a2505a0f9bcd174db48f131e54bfa8fe68"}}},"nbformat":4,"nbformat_minor":2}
